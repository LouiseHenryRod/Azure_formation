{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data in Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction DataStores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: encapsule info nécessaire pour se connecter aux sources de données\n",
    "\n",
    "Utilisations possibles: \n",
    "- integrer des données dans une experience\n",
    "- écrire/stocker des résultats d'une expérience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type de datastores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datastores crées pour de multiple type de source de données:\n",
    "- Azure Storage (blob and file containers\n",
    "- Azure Data Lake Storage \n",
    "- Az SQL Datrabase\n",
    "- Az Databricks file systm (DBFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilisation de datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enregistrement de datastores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 manières de créerun datastores : \n",
    "- 1 click bouton\n",
    "- 2 Via SDK python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici le code enregistr un Azure Storage blob contanie en tant que datastore du nom de blob_data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Register a new datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(workspace=ws,\n",
    "    datastore_name='blob_data',  \n",
    "    container_name='data_container',\n",
    "    account_name='az_store_acct',\n",
    "    account_key='123456abcde789…')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerer ses datastores via Azure ML Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lister les noms de l'ensemble de ses datastores de son ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_name in ws.datastores:\n",
    "    print(ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenir la réference d'un datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_store = Datastore.get(ws, datastore_name='blob_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otbenir/retrouver le datastore créer par défaut intitulé workspaceblobstore dans son ET. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_store = ws.get_default_datastore()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changer le datastore par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.set_default_datastore('blob_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiliser un Datastores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- enregistrer et telecharger des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_ds.upload(src_dir='/files',\n",
    "               target_path='/data/files',\n",
    "               overwrite=True, show_progress=True)\n",
    "\n",
    "blob_ds.download(target_path='downloads',\n",
    "                 prefix='/data',\n",
    "                 show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation d'un datastores dans une expérience --> necessité de transmettre une référence de données au script\n",
    "La référence de données est configurée  selon le mode d'accès:\n",
    "\n",
    "**Download**: The contents of the path associated with the data reference is downloaded to the compute context where the experiment is running.\n",
    "\n",
    "**Upload**: The files generated by your experiment script are uploaded to the datastore after the run completes.\n",
    "\n",
    "**Mount**: The path on the datastore is mounted as remote storage in the experiment compute context, enabling the contents to be accessed remotely (note that this mode is only available when the experiment is run on a remote compute target - you cannot use this mode with local compute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ref = blob_ds.path('data/files').as_download(path_on_compute='training_data') #telechargement de la base de données dans datastors\n",
    "\n",
    "\n",
    "\n",
    "estimator = SKLearn(source_directory='experiment_folder',\n",
    "                    entry_script='training_script.py'\n",
    "                    compute_target='local',\n",
    "                    script_params = {'--data_folder': data_ref}) #ajout de la reference de la banque de données/ des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', type=str, dest='data_folder')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_files = os.listdir(args.data_folder) # utilisation de argparse pour faciliter l'accès rapide au datastore définit plus haut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are versioned packaged data objects that can be easily consumed in experiments and pipelines. Datasets are the recommended way to work with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types d'ensemble de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Tabulaires = données lues sous forme de tableau structurés\n",
    "- 2) Fichiers = données non strucutrées "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Création et enregistrement d'ensembles de données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Création de datasets** \n",
    "\n",
    "--> créer un ensemble de données à partir de fichiers individuels ou de chemins relatifs\n",
    "\n",
    "rmq: chemins peuvent inclure caractéristiques génériques exemple: /files/*.csv\n",
    "\n",
    "\n",
    "**2) Enregistrer le dataset dans l'ET**\n",
    "\n",
    "--> permet de rendre dispo pour une utilisation dans une expérience ou dans un pipeline de traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <font color='green'>Création + enregistrement de données tabulaires</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "csv_paths = [(blob_ds, 'data/files/current_data.csv'),  #L'ensemble de données de cet exemple inclut les données de deux chemins de fichier\n",
    "             (blob_ds, 'data/files/archive/*.csv')]\n",
    "tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)\n",
    "tab_ds = tab_ds.register(workspace=ws, name='csv_table')  #Après avoir créé l'ensemble de données, le code \n",
    "                                                          #l'enregistre dans l'espace de travail sous le nom csv_table .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <font color='green'>Création + enregistrement de données fichiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Récupération des données enregistré"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des données, 2 façons:\n",
    "The datasets dictionary attribute of a Workspace object.\n",
    "\n",
    "\n",
    "1- via le **dictionnaire des attributs du dataset** de l'objet de type **Workspace** \n",
    "2- La méthode **get_by_name** ou **get_by_id** de la classe **Dataset** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get a dataset from the workspace datasets collection  --> Méthode 1\n",
    "ds1 = ws.datasets['csv_table']\n",
    "\n",
    "# Get a dataset by name from the datasets class --> méthode 2\n",
    "ds2 = Dataset.get_by_name(ws, 'img_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datasets versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#versioning\n",
    "img_paths = [(blob_ds, 'data/files/images/*.jpg'),\n",
    "             (blob_ds, 'data/files/images/*.png')]\n",
    "file_ds = Dataset.File.from_files(path=img_paths)\n",
    "\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)  #--> versioning\n",
    "\n",
    "\n",
    "#retrouver une version\n",
    "img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)  # --> version 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiliser un dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Travailler avec un dataset directement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tab_ds.to_pandas_dataframe()  #tab_ds étant le base de données tééchargée au préalable \n",
    "# code to work with dataframe goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode to_path () pour renvoyer une liste des chemins de fichier encapsulés par l'ensemble de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in file_ds.to_path():\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Passer un ensemble de données à un script d'expérimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inclusion dans un estimateur\n",
    "\n",
    "Rmq : étant donné que le script devra fonctionner avec un objet Dataset , vous devez inclure le package **azureml-sdk** complet ou le package **azureml-dataprep** avec la bibliothèque supplémentaire **pandas** **dans l'environnement de calcul du script.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SKLearn( source_directory='experiment_folder',\n",
    "                     entry_script='training_script.py',\n",
    "                     compute_target='local',\n",
    "                     inputs=[tab_ds.as_named_input('csv_data')],\n",
    "                     pip_packages=['azureml-dataprep[pandas]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation de la base de donnée en format pandas dans script de l'experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Run.get_context()\n",
    "data = run.input_datasets['csv_data'].to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et lors de la transmission d'un ensemble de **données de fichier**, **spécifier le mode d'accès**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator( source_directory='experiment_folder',\n",
    "                     entry_script='training_script.py'\n",
    "                     compute_target='local',\n",
    "                     inputs=[img_ds.as_named_input('img_data').as_download(path_on_compute='data')],\n",
    "                     pip_packages=['azureml-dataprep[pandas]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
