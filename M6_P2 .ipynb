{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploiement de modèles Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definition **inférence**: l’inférence fait référence à l’utilisation d’un modèle entraîné pour prédire des étiquettes pour les nouvelles données sur lesquelles le modèle **n’a pas** été entraîné"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déploiement d'un modèle en tant que service en temps réel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Machine Learning:\n",
    "- utilise des conteneurs comme mécanisme de déploiement, en empaquetant le modèle et le code pour l’utiliser en tant qu’image \n",
    "- qui peut être déployée sur un conteneur dans la cible de calcul que vous avez choisie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapes pour déployer un modèle en tant que service d’inférence en temps réel:\n",
    "- 1) Inscrire un modèle entraîné\n",
    "- 2) définir une configuration d'inférence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inscription du modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel : register_modelou Model.register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=ws,\n",
    "                       model_name='classification_model',\n",
    "                       model_path='model.pkl', # local path\n",
    "                       description='A classification model')\n",
    "\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Définir une configuration d'inférence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cela deux choses :\n",
    "- 1) definir le script qui charge le modèle et l'applique aux nouvelles données\n",
    "- 2) définir l'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>1 - définition du script d'entrée ou script de scoring</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the registered model file and load it\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # Return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()\n",
    "\n",
    "\n",
    "'''      Deux fonctions \n",
    "\n",
    "init()  : appelée lorsque le service est initialisé.\n",
    "run(raw_data)  : appelée lorsque de nouvelles données sont envoyées au service.\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web service will be hosted in a container,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>2 - définition de l'environnement</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Add the dependencies for your model\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = 'service_files/env.yml'\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)\n",
    "\n",
    "\n",
    "'''\n",
    "- Utilisation de la classe CondaDependencies\n",
    "- Ajouter tous les autres packages requis\n",
    "- Sérialiser l’environnement en une chaîne \n",
    "- Enregistrer \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>3 - Combinaison des deux en un **InferenceConfig**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                              source_directory = 'service_files',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              conda_file=\"env.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Définir une configuration de déploiement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But: consiste à configurer le calcul sur lequel le service sera déployé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''exemple :\n",
    "\n",
    "Si on déploie sur un cluster AKS on doit pour celui-ci,\n",
    "1- créer le cluster (étape 1)\n",
    "2- créer une cible de calcul (étape 2)\n",
    "\n",
    "'''\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus') #étape 1\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config) #étape 2\n",
    "production_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Définition de leconfiguration de déploiement, qui définit la spécification de calcul spécifique à la \n",
    "cible pour le déploiement conteneurisé\n",
    "\n",
    "'''\n",
    "\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                              memory_gb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque pour ACI: vous n’avez pas besoin de créer explicitement une cible de calcul ACI\n",
    "                   vous devez utiliser la classe deploy_configuration de l’espace de nom      azureml.core.webservice.AciWebservice. \n",
    "                   vous pouvez utiliser l’espace de noms azureml.core.webservice.LocalWebservice pour configurer un service local basé sur Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Déploiement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = ws.models['classification_model']\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model],\n",
    "                       inference_config = classifier_inference_config, #définit pendant la configuration d'inférence \n",
    "                       deployment_config = classifier_deploy_config, #définit dans la confi de déploiement\n",
    "                       deployment_target = production_cluster) #définit dans la confi de déploiement ac config calcul\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque: Pour les services ACI ou locaux, vous pouvez omettre le paramètre deployment_target ou le définir sur None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACI c'est plus en test ou en développement et AZure Kubernetes Service pour la production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation d'un service d'inférence en teps réel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilisation du SDK Azure Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation du SDK Azure Machine Learning pour appeler un service web\n",
    "\n",
    "En règle générale, vous envoyez des données à la méthode run au format JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})                 \n",
    "\n",
    "# Call the web service, passing the input data\n",
    "response = service.run(input_data = json_data)    #service étant le modèle déployé et transformé en service\n",
    "\n",
    "# Get the predictions\n",
    "predictions = json.loads(response)\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilisation d'un REST endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En production, la plupart des applications clientes n’incluront pas le SDK Azure Machine Learning et utiliseront le service via son interface REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupérer la propriété scoring_uri de l’objet Webservice dans le SDK\n",
    "\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)\n",
    "\n",
    "\n",
    "# Une fois le point de terminaison connu, vous pouvez utiliser une requête HTTP POST avec des données JSON \n",
    "# pour appeler le service\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Authentification </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour restreindre l’accès à vos services en appliquant une authentification deux types d’authentification :\n",
    "\n",
    "- Clé : les demandes sont authentifiées en spécifiant la clé associée au service.\n",
    "- Jeton : les demandes sont authentifiées en fournissant un jeton JWT (JSON Web Token).\n",
    "\n",
    "Rmq: Par défaut, l’authentification est désactivée pour les services ACI et définie sur l’authentification basée sur les clés pour les services AKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''En supposant que vous disposez d’une session authentifiée établie avec l’espace de travail, \n",
    "vous pouvez récupérer les clés d’un service avec la méthode get_keys de l’objet WebService associé au service'''\n",
    "\n",
    "primary_key, secondary_key = service.get_keys()\n",
    "\n",
    "'''\n",
    "Pour l’authentification basée sur les jetons, votre application cliente doit utiliser l’authentification \n",
    "du principal du service pour vérifier son identité par le biais d’Azure AD (Azure Active Directory) et \n",
    "appeler la méthode get_token du service pour récupérer un jeton limité dans le temps.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''Puis appliquer lauthentification au rest endpoint'''\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { \"Content-Type\":\"application/json\",\n",
    "                    \"Authorization\":\"Bearer \" + key_or_token }\n",
    "\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résolution des problèmes de déploiement de service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vérifier l'état du service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#par examination de son état \n",
    "\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "# Get the deployed service\n",
    "service = AciWebservice(name='classifier-service', workspace=ws)\n",
    "\n",
    "# Check its state\n",
    "print(service.state)   #Pour un service opérationnel, l’état doit être Sain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Consulter les journaux de service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si un service n’est pas sain ou si vous rencontrez des erreurs lors de son utilisation CONSULTER LES JOURNAUX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Déployer sur un conteneur local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les erreurs de déploiement et d’exécution peuvent être plus faciles à diagnostiquer en déployant le service en tant que conteneur dans une instance Docker locale, pour pouvoir tester le service déployé localement à l’aide du SDK :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=8890)\n",
    "service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.run(input_data = json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "résoudre les problèmes d’exécution:\n",
    "- 1) en apportant des modifications au fichier de scoring référencé dans la configuration d’inférence\n",
    "- 2) puis en rechargeant le service sans le redéployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.reload()\n",
    "print(service.run(input_data = json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to make a change and redeploy, you may need to delete unhealthy service using the following code:\n",
    "#service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# This time our input is an array of two feature arrays\n",
    "x_new = [[2,180,74,24,21,23.9091702,1.488172308,22],\n",
    "         [0,148,58,11,179,39.19207553,0.160829008,45]]\n",
    "\n",
    "# Convert the array or arrays to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "# Get the predicted classes.\n",
    "predicted_classes = json.loads(predictions)\n",
    "   \n",
    "for i in range(len(x_new)):\n",
    "    print (\"Patient {}\".format(x_new[i]), predicted_classes[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's determine the URL to which these applications must submit their requests:\n",
    "\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)  #renvois un URL "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
